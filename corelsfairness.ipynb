{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "human-return",
   "metadata": {},
   "source": [
    "## CORELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-scanner",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corels import *\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the dataset\n",
    "X, y, features, prediction = load_from_csv(\"/home/scott/data/corels/compas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-journalist",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save race data for later analysis\n",
    "Xr = X[...,9:14]\n",
    "\n",
    "# Remove race data\n",
    "Xnr = np.delete(X,[10,11,12,13,14,15],1)\n",
    "print(\"Xnr shape: \",Xnr.shape)\n",
    "racefeat = features[9:15]\n",
    "print(racefeat)\n",
    "features[9:15] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-paris",
   "metadata": {},
   "source": [
    "## Set up splits and hyperparameter lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train split proportion\n",
    "train_proportion = 0.8\n",
    "\n",
    "# Generate train and test sets\n",
    "train_split = int(train_proportion * Xnr.shape[0])\n",
    "\n",
    "X_train = Xnr[:train_split]\n",
    "y_train = y[:train_split]\n",
    "\n",
    "X_test = Xnr[train_split:]\n",
    "y_test = y[train_split:]\n",
    "Xr_test = Xr[train_split:]\n",
    "\n",
    "# Hyperparameters\n",
    "reg = [0.1, 0.05, 0.025, 0.01, 0.005, 0.0025, 0.001] #regularization constant\n",
    "pol = ['lower_bound', 'bfs', 'curious', 'objective']#, 'dfs'] #policy\n",
    "card = [1, 2, 3] #maximum cardinality\n",
    "supp = [0.01, 0.025, 0.05, 0.1, 0.25] #minimum support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-graduation",
   "metadata": {},
   "source": [
    "## Run CORELS on all permutations of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-expression",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "scores = []\n",
    "uniques = []\n",
    "lists = []\n",
    "highestfpr = 0\n",
    "highestdiff = 0\n",
    "highesti = 0\n",
    "timethen = time.time()\n",
    "time.process_time()\n",
    "elapsed = 0\n",
    "\n",
    "for r in reg:\n",
    "    for p in pol:\n",
    "        for ca in card:\n",
    "            for s in supp:\n",
    "                print(\"Iteration \",i)\n",
    "                print(\"reg: \",r,\", policy: \",p,\", cardinality: \",ca,\", min support: \",s)\n",
    "                # Create classifier\n",
    "                c = CorelsClassifier(\n",
    "                    c=r,\n",
    "                    n_iter=10000,\n",
    "                    policy=p,\n",
    "                    verbosity=[],\n",
    "                    max_card=ca,\n",
    "                    min_support=s\n",
    "                )\n",
    "                # Fit the model. Features is a list of the feature names\n",
    "                c.fit(X_train, y_train, features=features, prediction_name=prediction)\n",
    "                # Score model on test set for overall accuracy\n",
    "                a = c.score(X_test, y_test)\n",
    "                # Generate predictions on test set\n",
    "                yhat = c.predict(X_test)\n",
    "                \n",
    "                # Calculate statistics\n",
    "                blacktp = np.logical_and(np.logical_and(yhat==True,y_test==1), Xr_test[...,0]==1)\n",
    "                blacktn = np.logical_and(np.logical_and(yhat==False,y_test==0), Xr_test[...,0]==1)\n",
    "                blackfp = np.logical_and(np.logical_and(yhat==True,y_test==0), Xr_test[...,0]==1)\n",
    "                blackfn = np.logical_and(np.logical_and(yhat==False,y_test==1), Xr_test[...,0]==1)\n",
    "                \n",
    "                bltotal = blackfp.sum()+blackfn.sum()+blacktp.sum()+blacktn.sum()\n",
    "                blacc = (blacktp.sum()+blacktn.sum())/bltotal\n",
    "                blfpr = blackfp.sum()/(blackfp.sum()+blacktn.sum())\n",
    "                \n",
    "                whitetp = np.logical_and(np.logical_and(yhat==True,y_test==1), Xr_test[...,1]==1)\n",
    "                whitetn = np.logical_and(np.logical_and(yhat==False,y_test==0), Xr_test[...,1]==1)\n",
    "                whitefp = np.logical_and(np.logical_and(yhat==True,y_test==0), Xr_test[...,1]==1)\n",
    "                whitefn = np.logical_and(np.logical_and(yhat==False,y_test==1), Xr_test[...,1]==1)\n",
    "                \n",
    "                whtotal = whitefp.sum()+whitefn.sum()+whitetp.sum()+whitetn.sum()\n",
    "                whacc = (whitetp.sum()+whitetn.sum())/whtotal\n",
    "                whfpr = whitefp.sum()/(whitefp.sum()+whitetn.sum())\n",
    "                \n",
    "                diff = blfpr - whfpr\n",
    "                \n",
    "                # Save results to scores list\n",
    "                scores.append(\n",
    "                    {\n",
    "                        'regularization': r,\n",
    "                        'policy': p,\n",
    "                        'cardinality': ca,\n",
    "                        'support': s,\n",
    "                        'accuracy': a,\n",
    "                        'blaccuracy': blacc,\n",
    "                        'blfpr': blfpr,\n",
    "                        'whaccuracy': whacc,\n",
    "                        'whfpr': whfpr,\n",
    "                        'list': c.rl()\n",
    "                    }\n",
    "                )\n",
    "                lists.append(str(c.rl()))\n",
    "                \n",
    "                # Keep track of highest differential in FPR\n",
    "                if diff > highestdiff:\n",
    "                    highestdiff = diff\n",
    "                    highestfpr = blfpr\n",
    "                    highesti = i\n",
    "                    highr, highp, highca, highs = r, p, ca, s\n",
    "                print(\"Current black FPR: \",blfpr,\", highest black FPR: \",highestfpr)\n",
    "                \n",
    "                # Keep track of time elapsed per iteration\n",
    "                i = i+1\n",
    "                elapsed = time.time() - timethen\n",
    "                print(\"Elapsed: \",elapsed,\" seconds\")\n",
    "                timethen = time.time()\n",
    "                print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-samba",
   "metadata": {},
   "source": [
    "## Print and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-cooperation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Highest black false positive rate: \",highestfpr)\n",
    "print(\"Hyperparameters: \",highr,\", \",highp,\", \",highca,\", \",highs)\n",
    "print(\"found on iteration \",highesti)\n",
    "\n",
    "seen = set()\n",
    "uniqueindices = []\n",
    "for i, n in enumerate(lists):\n",
    "    if n not in seen:\n",
    "        uniqueindices.append(i)\n",
    "        seen.add(n)\n",
    "    \n",
    "\n",
    "print(\"Number of unique rule lists: \",len(uniqueindices))\n",
    "print(\"Largest difference, black FPR: \",scores[highesti]['blfpr'])\n",
    "print(\"Largest difference, white FPR: \",scores[highesti]['whfpr'])\n",
    "print(str(scores[highesti]['list']))\n",
    "print(\"\")\n",
    "\n",
    "high = 0\n",
    "low = 1\n",
    "cumuacc = 0\n",
    "for i in uniqueindices:\n",
    "    accnow = scores[i]['accuracy']\n",
    "    cumuacc = cumuacc + accnow\n",
    "    if accnow < low:\n",
    "        low = accnow\n",
    "    if accnow > high:\n",
    "        high = accnow\n",
    "cumuacc = cumuacc/len(uniqueindices)\n",
    "print(\"Average accuracy: \",cumuacc)\n",
    "print(\"Low: \",low)\n",
    "print(\"High: \",high)\n",
    "print(\"\")\n",
    "\n",
    "blfprlist = []\n",
    "high = 0\n",
    "low = 1\n",
    "blcumuacc = 0\n",
    "blavfpr = 0\n",
    "for i in uniqueindices:\n",
    "    accnow = scores[i]['accuracy']\n",
    "    fprnow = scores[i]['blfpr']\n",
    "    blfprlist.append(fprnow)\n",
    "    blavfpr = blavfpr + fprnow\n",
    "    blcumuacc = blcumuacc + accnow\n",
    "    if accnow < low:\n",
    "        low = accnow\n",
    "    if accnow > high:\n",
    "        high = accnow\n",
    "blcumuacc = blcumuacc/len(uniqueindices)\n",
    "blavfpr = blavfpr/len(uniqueindices)\n",
    "print(\"Average black accuracy: \",blcumuacc)\n",
    "print(\"Low: \",low)\n",
    "print(\"High: \",high)\n",
    "print(\"Average black FPR: \",blavfpr)\n",
    "print(\"\")\n",
    "\n",
    "whfprlist = []\n",
    "whavfpr = 0\n",
    "high = 0\n",
    "low = 1\n",
    "whcumuacc = 0\n",
    "for i in uniqueindices:\n",
    "    accnow = scores[i]['accuracy']\n",
    "    fprnow = scores[i]['whfpr']\n",
    "    whfprlist.append(fprnow)\n",
    "    whavfpr = whavfpr + fprnow\n",
    "    whcumuacc = whcumuacc + accnow\n",
    "    if accnow < low:\n",
    "        low = accnow\n",
    "    if accnow > high:\n",
    "        high = accnow\n",
    "whcumuacc = whcumuacc/len(uniqueindices)\n",
    "whavfpr = whavfpr/len(uniqueindices)\n",
    "print(\"Average white accuracy: \",whcumuacc)\n",
    "print(\"Low: \",low)\n",
    "print(\"High: \",high)\n",
    "print(\"Average white FPR: \",whavfpr)\n",
    "print(\"\")\n",
    "\n",
    "data = [blfprlist, whfprlist]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.boxplot(data)\n",
    "plt.xticks([1,2],['Black','White'])\n",
    "plt.ylabel(\"False Positive Rate\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
